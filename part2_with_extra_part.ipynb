{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "import os\n",
    "import operator\n",
    "import ssl\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading the dataset\n",
    "\n",
    "This notebook should be exactly placed in the directory of the folder 'datasets_coursework1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './datasets_coursework1/bbc'\n",
    "def read_class_data(dataset_path, class_name):\n",
    "    path = dataset_path + '/' + class_name\n",
    "    files = os.listdir(path)\n",
    "    data = []\n",
    "    for file in files:\n",
    "        if not os.path.isdir(file):\n",
    "            f = open(path + '/' + file, encoding='latin')\n",
    "            content = f.read()\n",
    "            data.append(content)\n",
    "    return data\n",
    "    \n",
    "business_data = read_class_data(dataset_path, 'business')\n",
    "entertainment_data = read_class_data(dataset_path, 'entertainment')\n",
    "politics_data = read_class_data(dataset_path, 'politics')\n",
    "sport_data = read_class_data(dataset_path, 'sport')\n",
    "tech_data = read_class_data(dataset_path, 'tech')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all the dependancies of nltk\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_used_words(data):\n",
    "    words = []\n",
    "    for passage in data:\n",
    "        word_tokens = nltk.tokenize.word_tokenize(passage)\n",
    "        words += word_tokens\n",
    "    words = [word.lower() for word in words] # to lowercase\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english') and word.isalpha()]\n",
    "    return words\n",
    "\n",
    "# obatin the words used by each class of documents\n",
    "business_words = get_used_words(business_data)\n",
    "entertainment_words = get_used_words(entertainment_data)\n",
    "politics_words = get_used_words(politics_data)\n",
    "sport_words = get_used_words(sport_data)\n",
    "tech_words = get_used_words(tech_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the tf-idf values of the used words\n",
    "def get_tf_idf(list_words):\n",
    "    doc_frequency = defaultdict(int)\n",
    "    for word_list in list_words:\n",
    "        for i in word_list:\n",
    "            doc_frequency[i] += 1\n",
    "\n",
    "    # compute the term frequency of each word\n",
    "    word_tf = {}\n",
    "    for i in doc_frequency:\n",
    "        word_tf[i] = doc_frequency[i] / sum(doc_frequency.values())\n",
    "\n",
    "    # compute the inverse document frequency of each word\n",
    "    doc_num = len(list_words)\n",
    "    word_idf = {}  \n",
    "    word_doc = defaultdict(int)\n",
    "    for i in doc_frequency:\n",
    "        for j in list_words:\n",
    "            if i in j:\n",
    "                word_doc[i] += 1\n",
    "    for i in doc_frequency:\n",
    "        word_idf[i] = math.log(doc_num / (word_doc[i] + 1))\n",
    "\n",
    "    # compute the value of TF * IDF\n",
    "    word_tf_idf = {}\n",
    "    for i in doc_frequency:\n",
    "        word_tf_idf[i] = word_tf[i] * word_idf[i]\n",
    "\n",
    "    return word_tf_idf\n",
    "\n",
    "dictionary = get_tf_idf([business_words, entertainment_words, politics_words, sport_words, tech_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the top 2000 words as default\n",
    "def find_topwords(word_tf_idf, num=2000):\n",
    "    dict_feature_select = sorted(word_tf_idf.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return dict_feature_select[:num]\n",
    "\n",
    "list_vocabulary = [pair[0] for pair in find_topwords(dictionary)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_tokens(string):\n",
    "    sentence_split = nltk.tokenize.sent_tokenize(string)\n",
    "    list_tokens = []\n",
    "    sentence_tokens = []\n",
    "    for sentence in sentence_split:\n",
    "        list_tokens_sentence = nltk.tokenize.word_tokenize(sentence)\n",
    "        sentence_tokens.append(list_tokens_sentence)\n",
    "        for token in list_tokens_sentence:\n",
    "            list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    num_sentence = len(sentence_tokens)\n",
    "    num_vocab = len(list(set(list_tokens)))\n",
    "    return list_tokens, np.array(num_sentence), np.array(num_vocab)\n",
    "\n",
    "def get_vector_text(list_vocab,string):\n",
    "    vector_text = np.zeros(len(list_vocab))\n",
    "    list_tokens_string, num_sentence, num_vocab = get_list_tokens(string)\n",
    "    for i, word in enumerate(list_vocab):\n",
    "        if word in list_tokens_string:\n",
    "            vector_text[i] = list_tokens_string.count(word)\n",
    "    return vector_text, num_sentence, num_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three features are used: top 2000 words frequency vector, number of sentence of the passage and number of used words\n",
    "def creating_features(list_vocabulary, data):\n",
    "    features = []\n",
    "    for passage in data:\n",
    "        vector, num_sentence, num_vocab = get_vector_text(list_vocabulary, passage)\n",
    "        vector = np.append(vector, num_sentence)\n",
    "        vector = np.append(vector, num_vocab)\n",
    "        features.append(vector)\n",
    "    return features\n",
    "\n",
    "business_features = creating_features(list_vocabulary, business_data)\n",
    "entertainment_features = creating_features(list_vocabulary, entertainment_data)\n",
    "politics_features = creating_features(list_vocabulary, politics_data)\n",
    "sport_features = creating_features(list_vocabulary, sport_data)\n",
    "tech_features = creating_features(list_vocabulary, tech_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.517129351031216\n"
     ]
    }
   ],
   "source": [
    "# mix different class of documents\n",
    "all_features = np.array(\n",
    "    business_features + entertainment_features + politics_features + sport_features + tech_features)\n",
    "all_labels = [0 for i in range(len(business_features))] + [1 for i in range(len(entertainment_features))] + [2 for i in range(len(politics_features))] + [3 for i in range(len(sport_features))] + [4 for i in range(len(tech_features))]\n",
    "\n",
    "vector_features = all_features[:, 0:-2]\n",
    "length_features = all_features[:, -2:]\n",
    "\n",
    "# dimension reduction on the word vector features\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(vector_features)\n",
    "newX = pca.fit_transform(vector_features)\n",
    "print('Explained variance: ' + str(sum(pca.explained_variance_ratio_)))\n",
    "\n",
    "# normalizing the length_features\n",
    "min_value = np.min(length_features, 0)\n",
    "max_value = np.max(length_features, 0)\n",
    "length_features = (length_features - min_value) / max_value\n",
    "\n",
    "# concatenate all the features\n",
    "newX = np.concatenate((newX, length_features), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating SVM classifier and using stratified cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using cross validation: 0.8426921013093527\n",
      "F1 score using cross validation: 0.8209264929716177\n",
      "Precision using cross validation: 0.8981623168666774\n",
      "Recall using cross validation: 0.8356586826297617\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "\n",
    "stratified_folder = StratifiedKFold(n_splits=10, random_state=0, shuffle=False)\n",
    "\n",
    "accuracy = []\n",
    "f1_scores = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for train_index, test_index in stratified_folder.split(newX, all_labels):\n",
    "    model = svm.SVC(decision_function_shape='ovo')\n",
    "    model.fit([newX[i] for i in train_index], [all_labels[i] for i in train_index])\n",
    "    Y_pred = model.predict([newX[i] for i in test_index])\n",
    "    Y_true = np.array([all_labels[i] for i in test_index])\n",
    "    acc = accuracy_score(Y_true, Y_pred)\n",
    "    f1_scores = f1_score(Y_true, Y_pred, average='macro')\n",
    "    pr = precision_score(Y_true, Y_pred, average='macro')\n",
    "    re = recall_score(Y_true, Y_pred, average='macro')\n",
    "    accuracy.append(acc)\n",
    "    precision.append(pr)\n",
    "    recall.append(re)\n",
    "print('Accuracy using cross validation: '+ str(np.mean(accuracy)))\n",
    "print('F1 score using cross validation: '+ str(np.mean(f1_scores)))\n",
    "print('Precision using cross validation: '+ str(np.mean(precision)))\n",
    "print('Recall using cross validation: '+ str(np.mean(recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified instances in total of 221: 42\n",
      "\n",
      "The misclassified instances of class 0 is: 1.0 out of 51.0 instances.\n",
      "The misclassified distribution of class 0 is: [0. 0. 0. 0. 1.]\n",
      "\n",
      "The misclassified instances of class 1 is: 5.0 out of 38.0 instances.\n",
      "The misclassified distribution of class 1 is: [4. 0. 0. 0. 1.]\n",
      "\n",
      "The misclassified instances of class 2 is: 14.0 out of 41.0 instances.\n",
      "The misclassified distribution of class 2 is: [11.  0.  0.  1.  2.]\n",
      "\n",
      "The misclassified instances of class 3 is: 13.0 out of 51.0 instances.\n",
      "The misclassified distribution of class 3 is: [13.  0.  0.  0.  0.]\n",
      "\n",
      "The misclassified instances of class 4 is: 9.0 out of 40.0 instances.\n",
      "The misclassified distribution of class 4 is: [7. 2. 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obatin the index where the instance is misclassified\n",
    "misclassified_index = np.where(Y_true != Y_pred)[0]\n",
    "misclassified_pairs = [(Y_true[i], Y_pred[i]) for i in list(misclassified_index)]\n",
    "\n",
    "# calculate the statistics\n",
    "# (a) how many instances are misclassified in total\n",
    "# (b) what is the distribution of the misclassified instances for each class\n",
    "print('Misclassified instances in total of '+ str(len(Y_true)) + ': ' + str(len(misclassified_index)) + '\\n')\n",
    "test_number_of_each_class = np.zeros((5,))\n",
    "for i in range(len(Y_true)):\n",
    "    test_number_of_each_class[Y_true[i]] += 1\n",
    "misclassified_distr = np.zeros((5, 5))\n",
    "for i in range(len(misclassified_pairs)):\n",
    "    misclassified_distr[misclassified_pairs[i][0]][misclassified_pairs[i][1]] += 1\n",
    "for i in range(len(misclassified_distr)):\n",
    "    print('The misclassified instances of class ' + str(i) + ' is: ' + str(sum(misclassified_distr[i])) + ' out of ' + str(test_number_of_each_class[i]) + ' instances.')\n",
    "    print('The misclassified distribution of class ' + str(i) + ' is: ' + str(misclassified_distr[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Comparison\n",
    "In this part, we use several machine learning models to perform classification. They are SVM with kernals of 'rbf' (this is implemented in skearn SVC by default), 'linear' and 'poly' (the default is degree=3), Logistic Regression and Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using cross validation: 0.9316836879896758\n",
      "F1 score using cross validation: 0.9249669564776598\n",
      "Precision using cross validation: 0.9394650628399048\n",
      "Recall using cross validation: 0.9291714144925276\n"
     ]
    }
   ],
   "source": [
    "# (a) kernel='linear'\n",
    "accuracy = []\n",
    "f1_scores = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for train_index, test_index in stratified_folder.split(newX, all_labels):\n",
    "    model = svm.SVC(decision_function_shape='ovo', kernel='linear')\n",
    "    model.fit([newX[i] for i in train_index], [all_labels[i] for i in train_index])\n",
    "    Y_pred = model.predict([newX[i] for i in test_index])\n",
    "    Y_true = np.array([all_labels[i] for i in test_index])\n",
    "    acc = accuracy_score(Y_true, Y_pred)\n",
    "    f1_scores = f1_score(Y_true, Y_pred, average='macro')\n",
    "    pr = precision_score(Y_true, Y_pred, average='macro')\n",
    "    re = recall_score(Y_true, Y_pred, average='macro')\n",
    "    accuracy.append(acc)\n",
    "    precision.append(pr)\n",
    "    recall.append(re)\n",
    "print('Accuracy using cross validation: '+ str(np.mean(accuracy)))\n",
    "print('F1 score using cross validation: '+ str(np.mean(f1_scores)))\n",
    "print('Precision using cross validation: '+ str(np.mean(precision)))\n",
    "print('Recall using cross validation: '+ str(np.mean(recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using cross validation: 0.3950309113163241\n",
      "F1 score using cross validation: 0.3799190532398679\n",
      "Precision using cross validation: 0.8356691632098755\n",
      "Recall using cross validation: 0.3689921253580922\n"
     ]
    }
   ],
   "source": [
    "# (b) kernel='poly'\n",
    "accuracy = []\n",
    "f1_scores = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for train_index, test_index in stratified_folder.split(newX, all_labels):\n",
    "    model = svm.SVC(decision_function_shape='ovo', kernel='poly')\n",
    "    model.fit([newX[i] for i in train_index], [all_labels[i] for i in train_index])\n",
    "    Y_pred = model.predict([newX[i] for i in test_index])\n",
    "    Y_true = np.array([all_labels[i] for i in test_index])\n",
    "    acc = accuracy_score(Y_true, Y_pred)\n",
    "    f1_scores = f1_score(Y_true, Y_pred, average='macro')\n",
    "    pr = precision_score(Y_true, Y_pred, average='macro')\n",
    "    re = recall_score(Y_true, Y_pred, average='macro')\n",
    "    accuracy.append(acc)\n",
    "    precision.append(pr)\n",
    "    recall.append(re)\n",
    "print('Accuracy using cross validation: '+ str(np.mean(accuracy)))\n",
    "print('F1 score using cross validation: '+ str(np.mean(f1_scores)))\n",
    "print('Precision using cross validation: '+ str(np.mean(precision)))\n",
    "print('Recall using cross validation: '+ str(np.mean(recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using cross validation: 0.9397716696466366\n",
      "F1 score using cross validation: 0.9277827363943573\n",
      "Precision using cross validation: 0.9464378877092294\n",
      "Recall using cross validation: 0.937115492489086\n"
     ]
    }
   ],
   "source": [
    "# (c) Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "accuracy = []\n",
    "f1_scores = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for train_index, test_index in stratified_folder.split(newX, all_labels):\n",
    "    model = LogisticRegression(multi_class='ovr')\n",
    "    model.fit([newX[i] for i in train_index], [all_labels[i] for i in train_index])\n",
    "    Y_pred = model.predict([newX[i] for i in test_index])\n",
    "    Y_true = np.array([all_labels[i] for i in test_index])\n",
    "    acc = accuracy_score(Y_true, Y_pred)\n",
    "    f1_scores = f1_score(Y_true, Y_pred, average='macro')\n",
    "    pr = precision_score(Y_true, Y_pred, average='macro')\n",
    "    re = recall_score(Y_true, Y_pred, average='macro')\n",
    "    accuracy.append(acc)\n",
    "    precision.append(pr)\n",
    "    recall.append(re)\n",
    "print('Accuracy using cross validation: '+ str(np.mean(accuracy)))\n",
    "print('F1 score using cross validation: '+ str(np.mean(f1_scores)))\n",
    "print('Precision using cross validation: '+ str(np.mean(precision)))\n",
    "print('Recall using cross validation: '+ str(np.mean(recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using cross validation: 0.8849864419133742\n",
      "F1 score using cross validation: 0.8745450376569437\n",
      "Precision using cross validation: 0.8874458868921268\n",
      "Recall using cross validation: 0.8853233752076916\n"
     ]
    }
   ],
   "source": [
    "# (d) Decision Tree\n",
    "from sklearn import tree\n",
    "\n",
    "accuracy = []\n",
    "f1_scores = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for train_index, test_index in stratified_folder.split(newX, all_labels):\n",
    "    model = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "    model.fit([newX[i] for i in train_index], [all_labels[i] for i in train_index])\n",
    "    Y_pred = model.predict([newX[i] for i in test_index])\n",
    "    Y_true = np.array([all_labels[i] for i in test_index])\n",
    "    acc = accuracy_score(Y_true, Y_pred)\n",
    "    f1_scores = f1_score(Y_true, Y_pred, average='macro')\n",
    "    pr = precision_score(Y_true, Y_pred, average='macro')\n",
    "    re = recall_score(Y_true, Y_pred, average='macro')\n",
    "    accuracy.append(acc)\n",
    "    precision.append(pr)\n",
    "    recall.append(re)\n",
    "print('Accuracy using cross validation: '+ str(np.mean(accuracy)))\n",
    "print('F1 score using cross validation: '+ str(np.mean(f1_scores)))\n",
    "print('Precision using cross validation: '+ str(np.mean(precision)))\n",
    "print('Recall using cross validation: '+ str(np.mean(recall)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
