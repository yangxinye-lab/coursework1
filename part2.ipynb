{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "import os\n",
    "import operator\n",
    "import ssl\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading the dataset\n",
    "\n",
    "This notebook should be exactly placed in the directory of the folder 'datasets_coursework1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './datasets_coursework1/bbc'\n",
    "def read_class_data(dataset_path, class_name):\n",
    "    path = dataset_path + '/' + class_name\n",
    "    files = os.listdir(path)\n",
    "    data = []\n",
    "    for file in files:\n",
    "        if not os.path.isdir(file):\n",
    "            f = open(path + '/' + file, encoding='latin')\n",
    "            content = f.read()\n",
    "            data.append(content)\n",
    "    return data\n",
    "    \n",
    "business_data = read_class_data(dataset_path, 'business')\n",
    "entertainment_data = read_class_data(dataset_path, 'entertainment')\n",
    "politics_data = read_class_data(dataset_path, 'politics')\n",
    "sport_data = read_class_data(dataset_path, 'sport')\n",
    "tech_data = read_class_data(dataset_path, 'tech')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all the dependancies of nltk\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_used_words(data):\n",
    "    words = []\n",
    "    for passage in data:\n",
    "        word_tokens = nltk.tokenize.word_tokenize(passage)\n",
    "        words += word_tokens\n",
    "    words = [word.lower() for word in words] # to lowercase\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english') and word.isalpha()]\n",
    "    return words\n",
    "\n",
    "# obatin the words used by each class of documents\n",
    "business_words = get_used_words(business_data)\n",
    "entertainment_words = get_used_words(entertainment_data)\n",
    "politics_words = get_used_words(politics_data)\n",
    "sport_words = get_used_words(sport_data)\n",
    "tech_words = get_used_words(tech_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the tf-idf values of the used words\n",
    "def get_tf_idf(list_words):\n",
    "    doc_frequency = defaultdict(int)\n",
    "    for word_list in list_words:\n",
    "        for i in word_list:\n",
    "            doc_frequency[i] += 1\n",
    "\n",
    "    # compute the term frequency of each word\n",
    "    word_tf = {}\n",
    "    for i in doc_frequency:\n",
    "        word_tf[i] = doc_frequency[i] / sum(doc_frequency.values())\n",
    "\n",
    "    # compute the inverse document frequency of each word\n",
    "    doc_num = len(list_words)\n",
    "    word_idf = {}  \n",
    "    word_doc = defaultdict(int)\n",
    "    for i in doc_frequency:\n",
    "        for j in list_words:\n",
    "            if i in j:\n",
    "                word_doc[i] += 1\n",
    "    for i in doc_frequency:\n",
    "        word_idf[i] = math.log(doc_num / (word_doc[i] + 1))\n",
    "\n",
    "    # compute the value of TF * IDF\n",
    "    word_tf_idf = {}\n",
    "    for i in doc_frequency:\n",
    "        word_tf_idf[i] = word_tf[i] * word_idf[i]\n",
    "\n",
    "    return word_tf_idf\n",
    "\n",
    "dictionary = get_tf_idf([business_words, entertainment_words, politics_words, sport_words, tech_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the top 2000 words as default\n",
    "def find_topwords(word_tf_idf, num=2000):\n",
    "    dict_feature_select = sorted(word_tf_idf.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return dict_feature_select[:num]\n",
    "\n",
    "list_vocabulary = [pair[0] for pair in find_topwords(dictionary)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_tokens(string):\n",
    "    sentence_split = nltk.tokenize.sent_tokenize(string)\n",
    "    list_tokens = []\n",
    "    sentence_tokens = []\n",
    "    for sentence in sentence_split:\n",
    "        list_tokens_sentence = nltk.tokenize.word_tokenize(sentence)\n",
    "        sentence_tokens.append(list_tokens_sentence)\n",
    "        for token in list_tokens_sentence:\n",
    "            list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    num_sentence = len(sentence_tokens)\n",
    "    num_vocab = len(list(set(list_tokens)))\n",
    "    return list_tokens, np.array(num_sentence), np.array(num_vocab)\n",
    "\n",
    "def get_vector_text(list_vocab,string):\n",
    "    vector_text = np.zeros(len(list_vocab))\n",
    "    list_tokens_string, num_sentence, num_vocab = get_list_tokens(string)\n",
    "    for i, word in enumerate(list_vocab):\n",
    "        if word in list_tokens_string:\n",
    "            vector_text[i] = list_tokens_string.count(word)\n",
    "    return vector_text, num_sentence, num_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three features are used: top 2000 words frequency vector, number of sentence of the passage and number of used words\n",
    "def creating_features(list_vocabulary, data):\n",
    "    features = []\n",
    "    for passage in data:\n",
    "        vector, num_sentence, num_vocab = get_vector_text(list_vocabulary, passage)\n",
    "        vector = np.append(vector, num_sentence)\n",
    "        vector = np.append(vector, num_vocab)\n",
    "        features.append(vector)\n",
    "    return features\n",
    "\n",
    "business_features = creating_features(list_vocabulary, business_data)\n",
    "entertainment_features = creating_features(list_vocabulary, entertainment_data)\n",
    "politics_features = creating_features(list_vocabulary, politics_data)\n",
    "sport_features = creating_features(list_vocabulary, sport_data)\n",
    "tech_features = creating_features(list_vocabulary, tech_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.5171559979188571\n"
     ]
    }
   ],
   "source": [
    "# mix different class of documents\n",
    "all_features = np.array(\n",
    "    business_features + entertainment_features + politics_features + sport_features + tech_features)\n",
    "all_labels = [0 for i in range(len(business_features))] + [1 for i in range(len(entertainment_features))] + [2 for i in range(len(politics_features))] + [3 for i in range(len(sport_features))] + [4 for i in range(len(tech_features))]\n",
    "\n",
    "vector_features = all_features[:, 0:-2]\n",
    "length_features = all_features[:, -2:]\n",
    "\n",
    "# dimension reduction on the word vector features\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(vector_features)\n",
    "newX = pca.fit_transform(vector_features)\n",
    "print('Explained variance: ' + str(sum(pca.explained_variance_ratio_)))\n",
    "\n",
    "# normalizing the length_features\n",
    "min_value = np.min(length_features, 0)\n",
    "max_value = np.max(length_features, 0)\n",
    "length_features = (length_features - min_value) / max_value\n",
    "\n",
    "# concatenate all the features\n",
    "newX = np.concatenate((newX, length_features), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating SVM classifier and using stratified cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using cross validation: 0.8471927111658053\n",
      "F1 score using cross validation: 0.8337885057335859\n",
      "Precision using cross validation: 0.9002797211160682\n",
      "Recall using cross validation: 0.8406508252753062\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "\n",
    "stratified_folder = StratifiedKFold(n_splits=10, random_state=0, shuffle=False)\n",
    "\n",
    "accuracy = []\n",
    "f1_scores = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for train_index, test_index in stratified_folder.split(newX, all_labels):\n",
    "    model = svm.SVC(decision_function_shape='ovo')\n",
    "    model.fit([newX[i] for i in train_index], [all_labels[i] for i in train_index])\n",
    "    Y_pred = model.predict([newX[i] for i in test_index])\n",
    "    Y_true = np.array([all_labels[i] for i in test_index])\n",
    "    acc = accuracy_score(Y_true, Y_pred)\n",
    "    f1_scores = f1_score(Y_true, Y_pred, average='macro')\n",
    "    pr = precision_score(Y_true, Y_pred, average='macro')\n",
    "    re = recall_score(Y_true, Y_pred, average='macro')\n",
    "    accuracy.append(acc)\n",
    "    precision.append(pr)\n",
    "    recall.append(re)\n",
    "print('Accuracy using cross validation: '+ str(np.mean(accuracy)))\n",
    "print('F1 score using cross validation: '+ str(np.mean(f1_scores)))\n",
    "print('Precision using cross validation: '+ str(np.mean(precision)))\n",
    "print('Recall using cross validation: '+ str(np.mean(recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
